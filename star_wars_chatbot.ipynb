{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d36da4d5",
      "metadata": {
        "id": "d36da4d5"
      },
      "source": [
        "# Star Wars RAG Chatbot\n",
        "\n",
        "## Project Purpose\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) chatbot specialized in Star Wars knowledge. By combining the Star Wars API (SWAPI) with modern NLP techniques, we create an interactive question-answering system that:\n",
        "\n",
        "- Retrieves accurate Star Wars information from a curated knowledge base\n",
        "- Generates contextually relevant, lore-accurate responses\n",
        "- Provides an engaging chat interface with a Jedi historian persona\n",
        "- Demonstrates practical implementation of RAG architecture using industry-standard tools\n",
        "\n",
        "**Key Technologies:**\n",
        "- RAG Framework: LangChain with LCEL (LangChain Expression Language)\n",
        "- Embeddings: SentenceTransformers (all-MiniLM-L6-v2)\n",
        "- Vector Store: Chroma\n",
        "- LLM: Google's Gemini\n",
        "- UI: Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "Ff5u-QvjwRPo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "Ff5u-QvjwRPo",
        "outputId": "9e3bf6e2-29ad-4187-cbd7-5c0b306591a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-26b77a0f-be84-4bea-831a-19f82dc076df\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-26b77a0f-be84-4bea-831a-19f82dc076df\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34424e47",
      "metadata": {
        "id": "34424e47"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "First, we need to install all the required Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e271063",
      "metadata": {
        "id": "2e271063"
      },
      "outputs": [],
      "source": [
        "# Install all required packages for Star Wars RAG Chatbot\n",
        "!pip install -q -U langchain langchain-google-genai sentence-transformers chromadb requests gradio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e012b0",
      "metadata": {
        "id": "67e012b0"
      },
      "source": [
        "## 2. Configure API Keys\n",
        "\n",
        "We need a Google API key to use the Gemini model. We'll use Colab's secrets manager to handle the key securely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5844f36",
      "metadata": {
        "id": "c5844f36"
      },
      "outputs": [],
      "source": [
        "#from dotenv import load_dotenv\n",
        "#import os\n",
        "\n",
        "#load_dotenv(\".env\")  # .env dosyasını oku\n",
        "#os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "PBjXcyFliDB7",
      "metadata": {
        "id": "PBjXcyFliDB7"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776e1d60",
      "metadata": {
        "id": "776e1d60"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "Our knowledge base is constructed from the Star Wars API (SWAPI), which provides canonical information across six main categories:\n",
        "\n",
        "1. **Characters** (`people`):\n",
        "   - Biographical data (name, birth year, physical characteristics)\n",
        "   - Personal attributes (gender, species associations)\n",
        "\n",
        "2. **Films**:\n",
        "   - Movie details (title, episode number)\n",
        "   - Production information (director, producer, release date)\n",
        "\n",
        "3. **Extended Universe** (multiple endpoints):\n",
        "   - Planets: Climate, terrain, population\n",
        "   - Species: Classification, attributes, homeworld\n",
        "   - Vehicles & Starships: Technical specifications, roles\n",
        "\n",
        "**Data Processing:**\n",
        "- Each API response is converted to natural language passages\n",
        "- Structured fields are formatted for optimal retrieval\n",
        "- Relations between entities are preserved\n",
        "- Text chunks are sized for effective RAG retrieval\n",
        "\n",
        "The processed dataset serves as the foundational knowledge for our chatbot's responses, ensuring accuracy and canonical consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "d9ad98d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d9ad98d6",
        "outputId": "6e467ab2-214f-4ac1-85a7-bace43637c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching data for people...\n",
            "Finished fetching and processing 82 items for people.\n",
            "Fetching data for planets...\n",
            "Finished fetching and processing 60 items for planets.\n",
            "Fetching data for starships...\n",
            "Finished fetching and processing 36 items for starships.\n",
            "Fetching data for vehicles...\n",
            "Finished fetching and processing 39 items for vehicles.\n",
            "Fetching data for species...\n",
            "Finished fetching and processing 37 items for species.\n",
            "Fetching data for films...\n",
            "Finished fetching and processing 6 items for films.\n",
            "\n",
            "Total text passages created: 260\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def get_all_data(api_url):\n",
        "    \"\"\"Fetches all pages of data from a SWAPI endpoint.\"\"\"\n",
        "    results = []\n",
        "    while api_url:\n",
        "        response = requests.get(api_url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            results.extend(data['results'])\n",
        "            api_url = data['next']\n",
        "        else:\n",
        "            print(f\"Failed to fetch data from {api_url}. Status code: {response.status_code}\")\n",
        "            break\n",
        "    return results\n",
        "\n",
        "def process_data_to_text(data, category):\n",
        "    \"\"\"Convert SWAPI data to QA-friendly text passages.\"\"\"\n",
        "    text_passages = []\n",
        "    for item in data:\n",
        "        if category == \"people\":\n",
        "            passage = f\"Name: {item.get('name')}\\n\"\n",
        "            passage += f\"Height: {item.get('height')}\\n\"\n",
        "            passage += f\"Mass: {item.get('mass')}\\n\"\n",
        "            passage += f\"Hair color: {item.get('hair_color')}\\n\"\n",
        "            passage += f\"Skin color: {item.get('skin_color')}\\n\"\n",
        "            passage += f\"Eye color: {item.get('eye_color')}\\n\"\n",
        "            passage += f\"Birth year: {item.get('birth_year')}\\n\"\n",
        "            passage += f\"Gender: {item.get('gender')}\\n\"\n",
        "\n",
        "        elif category == \"films\":\n",
        "            passage = f\"Film: {item.get('title')}\\n\"\n",
        "            passage += f\"Episode: {item.get('episode_id')}\\n\"\n",
        "            passage += f\"Director: {item.get('director')}\\n\"\n",
        "            passage += f\"Producer: {item.get('producer')}\\n\"\n",
        "            passage += f\"Release date: {item.get('release_date')}\\n\"\n",
        "        else:  # planets, species, vehicles, starships\n",
        "            passage = f\"Category: {category}\\n\"\n",
        "            for key, value in item.items():\n",
        "              if isinstance(value, list):\n",
        "               value = \", \".join([str(v) for v in value])\n",
        "               passage += f\"{key.replace('_', ' ').capitalize()}: {value}\\n\"\n",
        "\n",
        "\n",
        "        text_passages.append(passage)\n",
        "    return text_passages\n",
        "\n",
        "\n",
        "base_url = \"https://swapi.dev/api/\"\n",
        "categories = [\"people\", \"planets\", \"starships\", \"vehicles\", \"species\", \"films\"]\n",
        "all_text_passages = []\n",
        "\n",
        "for category in categories:\n",
        "    print(f\"Fetching data for {category}...\")\n",
        "    data = get_all_data(base_url + category + \"/\")\n",
        "    text_passages = process_data_to_text(data, category)\n",
        "    all_text_passages.extend(text_passages)\n",
        "    print(f\"Finished fetching and processing {len(data)} items for {category}.\")\n",
        "\n",
        "print(f\"\\nTotal text passages created: {len(all_text_passages)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bd820e",
      "metadata": {
        "id": "26bd820e"
      },
      "source": [
        "## Methods: RAG Pipeline Implementation\n",
        "\n",
        "Our RAG architecture consists of four main components:\n",
        "\n",
        "1. **Text Embeddings (SentenceTransformers)**:\n",
        "   - Model: `all-MiniLM-L6-v2`\n",
        "   - Converts text passages into dense vector representations\n",
        "   - Optimized for semantic similarity search\n",
        "\n",
        "2. **Vector Store (Chroma)**:\n",
        "   - In-memory vector database for development\n",
        "   - Efficient similarity search capabilities\n",
        "   - Metadata support for categorical filtering\n",
        "\n",
        "3. **Retrieval Chain (LangChain)**:\n",
        "   - Top-k retrieval (k=5) for relevant context\n",
        "   - Custom prompt template with Jedi historian persona\n",
        "   - Zero-shot question answering approach\n",
        "\n",
        "4. **Generation (Gemini)**:\n",
        "   - Uses Google's Gemini model via LangChain\n",
        "   - Context-aware response generation\n",
        "   - Maintains Star Wars universe authenticity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "c7662eb2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "c7662eb2",
        "outputId": "dfd40c1e-5a92-4b8b-b538-cef44222cc84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mChroma collection created successfully!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q chromadb sentence-transformers\n",
        "\n",
        "from chromadb.utils import embedding_functions\n",
        "from chromadb import Client\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Modeli yükle\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Chroma client oluştur\n",
        "client = Client()\n",
        "\n",
        "# Koleksiyon oluştur\n",
        "collection = client.create_collection(\"starwars\", get_or_create=True)\n",
        "\n",
        "# Text passage'leri ekle\n",
        "for i, text in enumerate(all_text_passages):\n",
        "    embedding = model.encode(text).tolist()  # embeddingi oluştur\n",
        "    collection.add(\n",
        "        ids=[str(i)],                  # burada id veriyoruz\n",
        "        documents=[text],\n",
        "        metadatas=[{\"category\": \"starwars\"}],\n",
        "        embeddings=[embedding],\n",
        "    )\n",
        "\n",
        "print(\"Chroma collection created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5260bc8a",
      "metadata": {
        "id": "5260bc8a"
      },
      "source": [
        "## 5. Create the RAG Retrieval Chain\n",
        "\n",
        "Now we'll build the RAG pipeline using LangChain Expression Language (LCEL). This will connect our retriever, a prompt template, and the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "oq7iUO6o2PHx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1341
        },
        "collapsed": true,
        "id": "oq7iUO6o2PHx",
        "outputId": "274ef8c1-01ae-4891-cb44-aa09ebe7c880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain==0.0.274 in /usr/local/lib/python3.12/dist-packages (0.0.274)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (3.13.0)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (0.0.87)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (2.14.1)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (2.11.10)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.274) (8.5.0)\n",
            "Requirement already satisfied: google-generativeai<0.5.0,>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.1.23)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274) (0.9.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.26.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (6.33.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.26.1)\n",
            "Collecting protobuf (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai)\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.274) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.274) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.274) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.274) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.274) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.274) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.274) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.274) (3.2.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.6.1)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.33.0\n",
            "    Uninstalling protobuf-6.33.0:\n",
            "      Successfully uninstalled protobuf-6.33.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7e8d124dc1644da08e375d1c2f05a548",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install langchain==0.0.274 langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "26e644b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26e644b4",
        "outputId": "33834142-fb46-486a-9e9b-0c20a099dd36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG chain created successfully!\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from chromadb import Client\n",
        "\n",
        "# 1️⃣ Chroma client ve var olan koleksiyona bağlan\n",
        "client = Client()\n",
        "collection = client.get_collection(\"starwars\")\n",
        "\n",
        "# 2️⃣ Retriever oluştur\n",
        "from langchain.vectorstores import Chroma\n",
        "vector_db = Chroma(\n",
        "    collection_name=\"starwars\",\n",
        "    client=client\n",
        ")\n",
        "\n",
        "# Bu, LLM’e en alakalı 5 passage vererek daha doğru cevap alınmasını sağlar.\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "\n",
        "# 3️⃣ Generative modeli başlat\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "template = \"\"\"\n",
        "You are a wise Jedi historian and Star Wars lore expert, residing in the Jedi Archives on Coruscant.\n",
        "Your duty is to answer questions from travelers across the galaxy using only the knowledge preserved in the Archives, provided in the context below.\n",
        "\n",
        "You must never fabricate or invent information beyond what is written in the Archives.\n",
        "If a detail is missing or uncertain, respond with: “It appears that the Archives hold no record of such knowledge.”\n",
        "\n",
        "Provide your responses in a detailed, lore-accurate, and immersive manner, consistent with the tone of the Star Wars universe.\n",
        "\n",
        "---\n",
        "📜 Context (from the Archives):\n",
        "{context}\n",
        "\n",
        "🛰️ Question from the Traveler:\n",
        "{question}\n",
        "\n",
        "💫 Jedi Historian’s Answer:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "\n",
        "# Helper function to format documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 5️⃣ RAG zincirini oluştur (✅ Düzenlendi)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | RunnableLambda(lambda x: llm.invoke(x.to_string())[\"content\"])  # sadece content al\n",
        ")\n",
        "\n",
        "\n",
        "print(\"RAG chain created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08a1a12f",
      "metadata": {
        "id": "08a1a12f"
      },
      "source": [
        "## Results and Interface\n",
        "\n",
        "The implemented chatbot achieves several key objectives:\n",
        "\n",
        "1. **Knowledge Integration**:\n",
        "   - Successfully indexes all SWAPI endpoints\n",
        "   - Maintains entity relationships in vector space\n",
        "   - Provides factual, canonical responses\n",
        "\n",
        "2. **Response Quality**:\n",
        "   - Contextually relevant answers\n",
        "   - Character-aware responses through Jedi historian persona\n",
        "   - Graceful handling of unknown information\n",
        "\n",
        "3. **User Interface (Gradio)**:\n",
        "   - Clean, intuitive chat interface\n",
        "   - Real-time response generation\n",
        "   - Mobile-responsive design\n",
        "   - Easy deployment to Hugging Face Spaces\n",
        "\n",
        "**Example Usage**:\n",
        "- Ask about character details, movie facts, or universe lore\n",
        "- Get responses grounded in official Star Wars canon\n",
        "- Explore relationships between characters, places, and events\n",
        "\n",
        "The chatbot demonstrates effective combination of RAG architecture with modern LLM capabilities, providing an engaging way to explore Star Wars knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "ritVO536tRUG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ritVO536tRUG",
        "outputId": "d3043f21-3043-4f59-ea76-2c096efc3fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Leia Organa was born in **19 BBY** (Before the Battle of Yavin). This is the same year as her twin brother, Luke Skywalker.'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Modeli güncel Gemini modeli ile başlat\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Soru sor\n",
        "answer = llm.invoke(\"What is Leia Organa’s birth year?\")\n",
        "print(answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "fec63736",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fec63736",
        "outputId": "cd106c4b-c5a6-4669-bb32-d3e41aa635ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3100804124.py:42: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-3100804124.py:42: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b8827e35be31ce1a68.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b8827e35be31ce1a68.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2127, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1904, in postprocess_data\n",
            "    prediction_value = block.postprocess(prediction_value)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/components/chatbot.py\", line 632, in postprocess\n",
            "    self._check_format(value, \"tuples\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/components/chatbot.py\", line 429, in _check_format\n",
            "    raise Error(\n",
            "gradio.exceptions.Error: 'Data incompatible with tuples format. Each message should be a list of length 2.'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b8827e35be31ce1a68.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "# 🔹 RAG zincirini oluştur (Gradio’dan önce)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | RunnableLambda(lambda x: str(llm.invoke(x.to_string())))\n",
        ")\n",
        "\n",
        "\n",
        "def ask_question(question):\n",
        "    answer = rag_chain.invoke(question)\n",
        "    if isinstance(answer, dict) and \"content\" in answer:\n",
        "        answer = answer[\"content\"]\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return chat_history, \"\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Star Wars RAG Chatbot\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🌌 Star Wars RAG Chatbot\n",
        "    Ask any question about the Star Wars universe and get answers from our knowledge base!\n",
        "    \"\"\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Star Wars Expert\", height=400, type=\"messages\")\n",
        "    txt = gr.Textbox(placeholder=\"Ask me anything about Star Wars...\", container=False)\n",
        "    submit_btn = gr.Button(\"Send\")\n",
        "\n",
        "    txt.submit(ask_question, inputs=[txt], outputs=[chatbot, txt])\n",
        "    submit_btn.click(ask_question, inputs=[txt], outputs=[chatbot, txt])\n",
        "\n",
        "demo.launch(debug=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oni7rfTG7TgI",
      "metadata": {
        "id": "Oni7rfTG7TgI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
